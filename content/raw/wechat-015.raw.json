{
  "translationKey": "wechat-015",
  "title": "北京到上海的距离：大语言模型的文本向量化和RAG增强向量数据库",
  "date": "2025-10-16T00:00:00.000Z",
  "html": null,
  "markdown_raw": "我们总在说“大语言模型很聪明”，\n\n它能写文章、回答问题、编代码，甚至能进行一些逻辑推理。\n\n但其实，这背后的原理并不神秘：\n\n👉 它只是把一段文字变成一串数字，\n\n👉 然后在一个高维空间里“计算距离”。\n\n这句话听起来有点抽象，但我们用一个小例子，你马上就能明白了👇\n\n一、语言也能变成“坐标”比如这句话：\n\n曹操是怎么死的？模型并不会像人一样“理解”这句话，\n\n而是先把它转换成一个固定长度的向量，\n\n比如 1536 维 的坐标[ 0.01234, -0.01891, 0.00023, …, 0.07112 ]\n\n👉 不管你原来输入的句子有多长，转换后都是 1536 个数。\n\n这意味着每一句话，在语义空间中都拥有了一个“地址”。\n\n这一步叫做 文本向量化（Text Embedding），\n\n是现代大语言模型理解语言的第一步。\n\n二、用北京和上海来举个例子 🏙️想象我们生活的地球就是一个二维平面。\n\n每个城市，都有自己对应的经纬坐标。\n\n假设北京的“坐标”是 (1, 1)，\n\n上海的“坐标”是 (4, 5)。\n\n我们就可以用欧几里得距离公式计算它们之间的“距离”👇\n\n(4 - 1)^2 + (5 - 1)^2\n= 3^2 + 4^2\n= 9 + 16\n= 25\n开根号(25) = 5\n\n👉 距离越近，语义越相似\n\n👉 距离越远，语义差异越大\n\n这就是“向量距离”的基本思想。\n\n真实的大语言模型用的是 1536 维，但数学原理是一样的。\n\n只不过二维的“北京—上海”更容易让人直观理解。\n\n三、RAG：外挂“知识库” 🧠📚很多人误以为：\n\n大模型 = 知识库其实不对！\n\n大语言模型：负责理解语言、生成答案\n\n向量数据库：负责存储和检索资料\n\n比如你问：\n\n曹操是怎么死的？AI 的工作流程其实是：\n\n把你的问题向量化\n\n在 向量数据库 中找到与这个问题“语义距离”最近的内容（比如“曹操病逝于洛阳”）\n\n把这些资料 + 你的问题，一起交给大模型\n\n由大模型生成自然语言答案\n\n这种技术叫做 RAG（Retrieval-Augmented Generation，检索增强生成），\n\n它的好处是 👉 不用重新训练模型，就能让 AI “知道”你的本地知识。\n\n比如：企业资料库、专业文档、历史档案，都能这样接入。\n\n四、Transformer：模型的“思考”层 🧮在获得输入后，大语言模型的内部并不是“魔法”，\n\n而是由一层又一层的 Transformer 结构组成（通常 20 层以上）。\n\n每一层都在提炼和抽象语义，\n\n就像人脑在不断“加工”信息。\n\n最终，模型会在 1536 维的语义空间中，\n\n找到与你问题最接近的“知识点”，再把它转化成自然语言输出。\n\n五、为什么是 1536 维？ 🤔二维空间可以表示北京和上海的地理位置；\n\n但语言比地理信息复杂得多。\n\n一段话中，可能同时包含：\n\n时间\n\n地点\n\n主语\n\n情绪\n\n语法结构\n\n隐含关系\n\n二维根本不够用，所以模型选择了一个高维空间，比如 1536 维。\n\n这样可以更精准地刻画语义差异。\n\n距离越近 → 语义越接近\n\n距离越远 → 意思差别越大\n\n这就是“语义嵌入”（Semantic Embedding）的本质。\n\n六、小结 📝🧭 模型先把文本转换成 向量\n\n📏 语义相似 = 向量距离近\n\n📚 向量数据库负责 快速检索\n\n🧠 RAG 技术让模型拥有“外挂知识库”\n\n🧮 Transformer 负责 语义理解与生成\n\n📌 所以，当你在和 AI 聊天时，它是在一个 1536 维的空间里，找到了离你问题“最近的那个点”，然后用自然语言表达出来。\n"
}
