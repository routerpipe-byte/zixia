---
title: 北京到上海的距离：大语言模型的文本向量化和RAG增强向量数据库
date: 2025-10-16T00:00:00.000Z
draft: false
translationKey: wechat-015
slug: rag
---
我们总在说“大语言模型很聪明”，

它能写文章、回答问题、编代码，甚至能进行一些逻辑推理。

但其实，这背后的原理并不神秘：

👉 它只是把一段文字变成一串数字，

👉 然后在一个高维空间里“计算距离”。

这句话听起来有点抽象，但我们用一个小例子，你马上就能明白了👇

一、语言也能变成“坐标”比如这句话：

曹操是怎么死的？模型并不会像人一样“理解”这句话，

而是先把它转换成一个固定长度的向量，

比如 1536 维 的坐标[ 0.01234, -0.01891, 0.00023, …, 0.07112 ]

👉 不管你原来输入的句子有多长，转换后都是 1536 个数。

这意味着每一句话，在语义空间中都拥有了一个“地址”。

这一步叫做 文本向量化（Text Embedding），

是现代大语言模型理解语言的第一步。

二、用北京和上海来举个例子 🏙️想象我们生活的地球就是一个二维平面。

每个城市，都有自己对应的经纬坐标。

假设北京的“坐标”是 (1, 1)，

上海的“坐标”是 (4, 5)。

我们就可以用欧几里得距离公式计算它们之间的“距离”👇

(4 - 1)^2 + (5 - 1)^2
= 3^2 + 4^2
= 9 + 16
= 25
开根号(25) = 5

👉 距离越近，语义越相似

👉 距离越远，语义差异越大

这就是“向量距离”的基本思想。

真实的大语言模型用的是 1536 维，但数学原理是一样的。

只不过二维的“北京—上海”更容易让人直观理解。

三、RAG：外挂“知识库” 🧠📚很多人误以为：

大模型 = 知识库其实不对！

大语言模型：负责理解语言、生成答案

向量数据库：负责存储和检索资料

比如你问：

曹操是怎么死的？AI 的工作流程其实是：

把你的问题向量化

在 向量数据库 中找到与这个问题“语义距离”最近的内容（比如“曹操病逝于洛阳”）

把这些资料 + 你的问题，一起交给大模型

由大模型生成自然语言答案

这种技术叫做 RAG（Retrieval-Augmented Generation，检索增强生成），

它的好处是 👉 不用重新训练模型，就能让 AI “知道”你的本地知识。

比如：企业资料库、专业文档、历史档案，都能这样接入。

四、Transformer：模型的“思考”层 🧮在获得输入后，大语言模型的内部并不是“魔法”，

而是由一层又一层的 Transformer 结构组成（通常 20 层以上）。

每一层都在提炼和抽象语义，

就像人脑在不断“加工”信息。

最终，模型会在 1536 维的语义空间中，

找到与你问题最接近的“知识点”，再把它转化成自然语言输出。

五、为什么是 1536 维？ 🤔二维空间可以表示北京和上海的地理位置；

但语言比地理信息复杂得多。

一段话中，可能同时包含：

时间

地点

主语

情绪

语法结构

隐含关系

二维根本不够用，所以模型选择了一个高维空间，比如 1536 维。

这样可以更精准地刻画语义差异。

距离越近 → 语义越接近

距离越远 → 意思差别越大

这就是“语义嵌入”（Semantic Embedding）的本质。

六、小结 📝🧭 模型先把文本转换成 向量

📏 语义相似 = 向量距离近

📚 向量数据库负责 快速检索

🧠 RAG 技术让模型拥有“外挂知识库”

🧮 Transformer 负责 语义理解与生成

📌 所以，当你在和 AI 聊天时，它是在一个 1536 维的空间里，找到了离你问题“最近的那个点”，然后用自然语言表达出来。
